# ─────────────────────────────────────────────
# ASR-RAG Configuration
# ─────────────────────────────────────────────

# ── Model ──────────────────────────────────
model:
  name: "microsoft/Phi-3-mini-4k-instruct"
  quantization: "4bit"          # keep — llama.cpp uses Q4_K_M GGUF
  dtype: "float16"              # change bfloat16 → float16 (CPU compatible)
  max_new_tokens: 256           # reduce 512 → 256 to save RAM + speed up
  temperature: 0.1
  context_window: 2048          # reduce 4096 → 2048 (saves ~1 GB RAM on 8GB systems)

  # Laptop-specific model settings
  llm_backend: "llamacpp"       # "llamacpp" | "transformers"
  gguf_model_path: "models/llama-3-8b-instruct.Q4_K_M.gguf"
  n_gpu_layers: -1              # -1 = all layers on GPU (M2 Metal); 0 = CPU only (Intel i5)
  n_threads: 4                  # match your CPU core count (i5 11th Gen = 4 cores)
  n_batch: 1                    # process 1 sequence at a time on 8 GB RAM

# ── Embedding ──────────────────────────────
embedding:
  primary_model: "sentence-transformers/all-MiniLM-L6-v2"
  scientific_model: "allenai/scibert_scivocab_uncased"  # used for tech queries
  scientific_query_threshold: 0.7   # score above which SciBERT is used

# ── Vector Store ───────────────────────────
vector_store:
  provider: "chromadb"
  persist_dir: "data/chroma_db"
  collection_name: "asr_rag_corpus"
  hnsw_ef_construction: 100     # reduce 200 → 100 (less RAM, slightly less accuracy)
  hnsw_m: 8                     # reduce 16 → 8 (smaller in-memory index)

# ── Retrieval ──────────────────────────────
retrieval:
  top_k: 5
  alpha_dense: 0.70              # weight for dense retrieval
  alpha_sparse: 0.30             # weight for BM25
  bm25_k1: 1.5
  bm25_b: 0.75

# ── Layer 1: Self-Repair ───────────────────
self_repair:
  relevance_threshold: 0.60      # τ_r — triggers query reformulation
  consistency_threshold: 0.70   # τ_c — triggers regeneration
  max_iterations: 3              # N   — max repair cycles per query

# ── Layer 2: Feedback ──────────────────────
feedback:
  db_path: "data/feedback.db"
  log_implicit: true             # log reformulations, cycle counts, etc.
  log_explicit: true             # log user corrections and ratings

# ── Layer 3: Adaptation ────────────────────
adaptation:
  interval: 200                  # Δt — adapt every N queries
  min_samples_per_type: 20       # min queries per type before adapting
  success_rate_threshold: 0.70   # below this → trigger strategy update
  section_aware_chunking: true

# ── Chunking ───────────────────────────────
chunking:
  strategy: "section_aware"      # "section_aware" | "fixed"
  overlap: 64
  section_sizes:                 # tokens per paper section
    abstract: 250
    introduction: 512
    related_work: 512
    methods: 768
    results: 768
    conclusion: 300
    default: 512

# ── Evaluation ─────────────────────────────
evaluation:
  metrics: ["answer_relevancy", "faithfulness", "context_precision", "context_recall"]
  record_every_n_queries: 200
  random_seeds: [42, 123, 456]

# ── Paths ──────────────────────────────────
paths:
  data_dir: "data/"
  results_dir: "results/"
  logs_dir: "logs/"

# ── Laptop / Low-Memory Settings ───────────────
laptop:
  max_corpus_chunks: 2000        # limit corpus to 2000 chunks max (fits in ~3 GB RAM)
  embedding_batch_size: 8        # embed 8 passages at a time (not 64)
  use_disk_chroma: true          # keep ChromaDB on disk, not in memory
